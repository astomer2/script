{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/80, Accuracy: 0.3817\n",
      "Iteration 2/80, Accuracy: 0.3951\n",
      "Iteration 3/80, Accuracy: 0.4093\n",
      "Iteration 4/80, Accuracy: 0.4574\n",
      "Iteration 5/80, Accuracy: 0.4643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6/80, Accuracy: 0.5248\n",
      "Iteration 7/80, Accuracy: 0.5495\n",
      "Iteration 8/80, Accuracy: 0.5814\n",
      "Iteration 9/80, Accuracy: 0.6356\n",
      "Iteration 10/80, Accuracy: 0.6691\n",
      "Iteration 11/80, Accuracy: 0.7107\n",
      "Iteration 12/80, Accuracy: 0.7489\n",
      "Iteration 13/80, Accuracy: 0.7725\n",
      "Iteration 14/80, Accuracy: 0.8049\n",
      "Iteration 15/80, Accuracy: 0.8195\n",
      "Iteration 16/80, Accuracy: 0.8509\n",
      "Iteration 17/80, Accuracy: 0.8615\n",
      "Iteration 18/80, Accuracy: 0.8832\n",
      "Iteration 19/80, Accuracy: 0.8871\n",
      "Iteration 20/80, Accuracy: 0.8864\n",
      "Iteration 21/80, Accuracy: 0.8977\n",
      "Iteration 22/80, Accuracy: 0.8951\n",
      "Iteration 23/80, Accuracy: 0.9027\n",
      "Iteration 24/80, Accuracy: 0.8989\n",
      "Iteration 25/80, Accuracy: 0.9068\n",
      "Iteration 26/80, Accuracy: 0.9097\n",
      "Iteration 27/80, Accuracy: 0.9102\n",
      "Iteration 28/80, Accuracy: 0.9117\n",
      "Iteration 29/80, Accuracy: 0.9138\n",
      "Iteration 30/80, Accuracy: 0.9143\n",
      "Iteration 31/80, Accuracy: 0.9149\n",
      "Iteration 32/80, Accuracy: 0.9130\n",
      "Iteration 33/80, Accuracy: 0.9130\n",
      "Iteration 34/80, Accuracy: 0.9150\n",
      "Iteration 35/80, Accuracy: 0.9200\n",
      "Iteration 36/80, Accuracy: 0.9220\n",
      "Iteration 37/80, Accuracy: 0.9240\n",
      "Iteration 38/80, Accuracy: 0.9250\n",
      "Iteration 39/80, Accuracy: 0.9310\n",
      "Iteration 40/80, Accuracy: 0.9320\n",
      "Iteration 41/80, Accuracy: 0.9280\n",
      "Iteration 42/80, Accuracy: 0.9350\n",
      "Iteration 43/80, Accuracy: 0.9350\n",
      "Iteration 44/80, Accuracy: 0.9320\n",
      "Iteration 45/80, Accuracy: 0.9350\n",
      "Iteration 46/80, Accuracy: 0.9370\n",
      "Iteration 47/80, Accuracy: 0.9370\n",
      "Iteration 48/80, Accuracy: 0.9370\n",
      "Iteration 49/80, Accuracy: 0.9400\n",
      "Iteration 50/80, Accuracy: 0.9400\n",
      "Iteration 51/80, Accuracy: 0.9420\n",
      "Iteration 52/80, Accuracy: 0.9410\n",
      "Iteration 53/80, Accuracy: 0.9430\n",
      "Iteration 54/80, Accuracy: 0.9440\n",
      "Iteration 55/80, Accuracy: 0.9440\n",
      "Iteration 56/80, Accuracy: 0.9450\n",
      "Iteration 57/80, Accuracy: 0.9450\n",
      "Iteration 58/80, Accuracy: 0.9450\n",
      "Iteration 59/80, Accuracy: 0.9470\n",
      "Iteration 60/80, Accuracy: 0.9470\n",
      "Iteration 61/80, Accuracy: 0.9460\n",
      "Iteration 62/80, Accuracy: 0.9490\n",
      "Iteration 63/80, Accuracy: 0.9500\n",
      "Iteration 64/80, Accuracy: 0.9500\n",
      "Iteration 65/80, Accuracy: 0.9530\n",
      "Iteration 66/80, Accuracy: 0.9540\n",
      "Iteration 67/80, Accuracy: 0.9540\n",
      "Iteration 68/80, Accuracy: 0.9530\n",
      "Iteration 69/80, Accuracy: 0.9550\n",
      "Iteration 70/80, Accuracy: 0.9560\n",
      "Iteration 71/80, Accuracy: 0.9580\n",
      "Iteration 72/80, Accuracy: 0.9580\n",
      "Iteration 73/80, Accuracy: 0.9580\n",
      "Iteration 74/80, Accuracy: 0.9590\n",
      "Iteration 75/80, Accuracy: 0.9620\n",
      "Iteration 76/80, Accuracy: 0.9620\n",
      "Iteration 77/80, Accuracy: 0.9630\n",
      "Iteration 78/80, Accuracy: 0.9630\n",
      "Iteration 79/80, Accuracy: 0.9650\n",
      "Iteration 80/80, Accuracy: 0.9630\n",
      "Final Test Accuracy: 0.9622\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 生成模拟数据\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
    "X_train, X_pool, y_train, y_pool = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_pool_tensor = torch.FloatTensor(X_pool)\n",
    "y_pool_tensor = torch.LongTensor(y_pool)\n",
    "\n",
    "# 定义简单的神经网络\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 初始化神经网络和优化器\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 10\n",
    "output_size = 2\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# 初始化训练数据集和池子数据集\n",
    "train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "pool_dataset = CustomDataset(X_pool_tensor, y_pool_tensor)\n",
    "\n",
    "# 定义主动学习循环\n",
    "num_iterations = 80\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # 训练模型\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 在池子数据中选择不确定性最大的样本\n",
    "    with torch.no_grad():\n",
    "        pool_loader = DataLoader(pool_dataset, batch_size=batch_size, shuffle=False)\n",
    "        uncertainty_scores = []\n",
    "        for inputs, _ in pool_loader:\n",
    "            outputs = model(inputs)\n",
    "            uncertainty = -torch.max(nn.functional.softmax(outputs, dim=1), dim=1)[0]\n",
    "            uncertainty_scores.extend(uncertainty.numpy())\n",
    "\n",
    "        # 选择不确定性最大的样本\n",
    "        selected_indices = np.argsort(uncertainty_scores)[:batch_size]\n",
    "\n",
    "    # 将选择的样本添加到训练数据集中，并从池子中移除\n",
    "    for idx in selected_indices:\n",
    "        if idx < len(pool_dataset):\n",
    "            train_dataset.features = torch.cat([train_dataset.features, pool_dataset.features[idx].unsqueeze(0)], dim=0)\n",
    "            train_dataset.labels = torch.cat([train_dataset.labels, pool_dataset.labels[idx].unsqueeze(0)], dim=0)\n",
    "\n",
    "            pool_dataset.features = torch.cat([pool_dataset.features[:idx], pool_dataset.features[idx + 1:]], dim=0)\n",
    "            pool_dataset.labels = torch.cat([pool_dataset.labels[:idx], pool_dataset.labels[idx + 1:]], dim=0)\n",
    "\n",
    "    # 打印每次迭代的准确性\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Iteration {i + 1}/{num_iterations}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 最终测试模型的性能\n",
    "test_dataset = CustomDataset(X_pool_tensor, y_pool_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AmberTools20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
